{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 1. ENVIRONMENT SETUP\nInstall required libraries, check for GPU availability, and authenticate using environment variables."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install runtime dependencies for fine tuning\n!pip install -q unsloth==0.5.0 datasets evaluate rouge-score peft accelerate bitsandbytes transformers==4.41.2 trl gradio==4.36.1 --extra-index-url https://download.pytorch.org/whl/cu121\n\nimport os\nimport platform\nimport torch\n\n# Detect the available device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Running on device: {device}\")\nprint(f\"Python version: {platform.python_version()}\")\n\n# Authenticate with Hugging Face using an environment variable\nhf_token = os.getenv(\"HF_TOKEN\", \"\")\nif hf_token:\n    from huggingface_hub import login\n    login(token=hf_token)\nelse:\n    print(\"HF_TOKEN environment variable not set. Skipping login.\")\n\n# Authenticate with Weights and Biases using an environment variable\nwandb_api_key = os.getenv(\"WANDB_API_KEY\", \"\")\nif wandb_api_key:\n    import wandb\n    wandb.login(key=wandb_api_key)\nelse:\n    print(\"WANDB_API_KEY environment variable not set. W&B login skipped.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 2. CONFIGURATION\nDefine base model, Hugging Face repo, Weights and Biases project, and core training hyperparameters."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Base 4 bit model from Unsloth for efficient fine tuning\nMODEL_NAME_BASE = \"unsloth/Meta-Llama-3-8B-Instruct-bnb-4bit\"\nHF_REPO_ID = os.getenv(\"HF_REPO_ID\", \"your-username/llama3-medical-cot\")\nWANDB_PROJECT = os.getenv(\"WANDB_PROJECT\", \"llama3-medical-cot\")\n\nhyperparameters = {\n    \"num_train_epochs\": 1,\n    \"train_batch_size\": 4,\n    \"gradient_accumulation_steps\": 4,\n    \"learning_rate\": 2e-4,\n    \"max_length\": 2048,\n    \"warmup_ratio\": 0.03,\n    \"weight_decay\": 0.01,\n    \"logging_steps\": 10,\n}\n\nprint(f\"Base model: {MODEL_NAME_BASE}\")\nprint(f\"Hugging Face repo ID: {HF_REPO_ID}\")\nprint(f\"W&B project: {WANDB_PROJECT}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 3. DATASET PREPARATION\nLoad pubmed_qa (pqa_labeled), build train and validation splits, and format into <think> and <response> text pairs."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from datasets import load_dataset\n\n# Load the labeled PubMedQA split\nraw_dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n\n# Create validation split from the training set\nsplit_dataset = raw_dataset[\"train\"].train_test_split(test_size=100, seed=42)\ntrain_dataset = split_dataset[\"train\"]\nval_dataset = split_dataset[\"test\"]\n\n# Format each sample into reasoning and answer fields\nsystem_prompt = (\n    \"You are a medical reasoning assistant. Provide step by step clinical thinking \"\n    \"before giving a concise answer.\"\n)\n\ndef format_sample(example):\n    context = example.get(\"context\", \"\")\n    question = example.get(\"question\", \"\")\n    long_answer = example.get(\"long_answer\", \"\")\n    thought = f\"<think> {system_prompt} Context: {context} Question: {question}\".strip()\n    response = f\"<response> {long_answer}\".strip()\n    return {\n        \"prompt\": thought,\n        \"response\": response,\n        \"text\": f\"{thought}\n{response}\"\n    }\n\ntrain_dataset = train_dataset.map(format_sample, remove_columns=train_dataset.column_names)\nval_dataset = val_dataset.map(format_sample, remove_columns=val_dataset.column_names)\n\nprint(train_dataset[0])\nprint(f\"Train size: {len(train_dataset)} | Validation size: {len(val_dataset)}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 4. TOKENIZATION\nLoad tokenizer and tokenize the formatted datasets with padding and truncation."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from transformers import AutoTokenizer\n\n# Load tokenizer from the base model\nbase_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_BASE, use_fast=True)\nif base_tokenizer.pad_token is None:\n    base_tokenizer.pad_token = base_tokenizer.eos_token\n\n# Tokenization function with truncation and padding\n\ndef tokenize_batch(batch):\n    return base_tokenizer(\n        batch[\"text\"],\n        max_length=hyperparameters[\"max_length\"],\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\",\n    )\n\ntrain_tokenized = train_dataset.map(tokenize_batch, batched=True, remove_columns=train_dataset.column_names)\nval_tokenized = val_dataset.map(tokenize_batch, batched=True, remove_columns=val_dataset.column_names)\n\nprint(train_tokenized)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 5. BASELINE EVALUATION\nGenerate answers with the base model and compute ROUGE L on a small validation subset."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import evaluate\nfrom transformers import AutoModelForCausalLM, pipeline\n\n# Load the base model for inference\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME_BASE,\n    device_map=\"auto\",\n)\n\ntext_generation = pipeline(\n    task=\"text-generation\",\n    model=base_model,\n    tokenizer=base_tokenizer,\n    max_new_tokens=128,\n    do_sample=False,\n)\n\n# Evaluate on a small subset for speed\nsubset = val_dataset.select(range(min(20, len(val_dataset))))\nrouge = evaluate.load(\"rouge\")\n\npreds = []\nrefs = []\nfor example in subset:\n    generated = text_generation(example[\"prompt\"], return_full_text=False)[0][\"generated_text\"]\n    preds.append(generated)\n    refs.append(example[\"response\"])\n\nbaseline_scores = rouge.compute(predictions=preds, references=refs, use_aggregator=True)\nprint(\"Baseline ROUGE-L:\", baseline_scores.get(\"rougeL\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 6. LORA FINE TUNING\nLoad the model with Unsloth, apply LoRA using PEFT, and train with gradient accumulation and mixed precision while logging to Weights and Biases."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from unsloth import FastLanguageModel\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# Load model in 4 bit for efficient training\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=MODEL_NAME_BASE,\n    max_seq_length=hyperparameters[\"max_length\"],\n    load_in_4bit=True,\n    dtype=None,\n)\n\n# Ensure tokenizer padding token is defined\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nFastLanguageModel.get_peft_model(model, lora_config)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"outputs\",\n    num_train_epochs=hyperparameters[\"num_train_epochs\"],\n    per_device_train_batch_size=hyperparameters[\"train_batch_size\"],\n    gradient_accumulation_steps=hyperparameters[\"gradient_accumulation_steps\"],\n    learning_rate=hyperparameters[\"learning_rate\"],\n    weight_decay=hyperparameters[\"weight_decay\"],\n    warmup_ratio=hyperparameters[\"warmup_ratio\"],\n    logging_steps=hyperparameters[\"logging_steps\"],\n    fp16=torch.cuda.is_available(),\n    bf16=False,\n    report_to=[\"wandb\"] if wandb_api_key else [],\n    run_name=WANDB_PROJECT,\n    max_grad_norm=1.0,\n    save_strategy=\"no\",\n)\n\n# Supervised fine tuning trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=hyperparameters[\"max_length\"],\n    args=training_args,\n)\n\ntrain_result = trainer.train()\nprint(train_result)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 7. SAVE AND PUSH\nSave the LoRA adapter and tokenizer, then push them to Hugging Face without the base model weights."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from huggingface_hub import HfApi\n\nadapter_dir = \"lora_adapter\"\ntokenizer_dir = \"lora_tokenizer\"\n\n# Save adapter and tokenizer locally\ntrainer.model.save_pretrained(adapter_dir)\ntokenizer.save_pretrained(tokenizer_dir)\n\n# Push to the specified repository if credentials are available\nif hf_token and HF_REPO_ID:\n    api = HfApi()\n    api.create_repo(HF_REPO_ID, exist_ok=True, token=hf_token)\n    api.upload_folder(folder_path=adapter_dir, repo_id=HF_REPO_ID, token=hf_token)\n    api.upload_folder(folder_path=tokenizer_dir, repo_id=HF_REPO_ID, token=hf_token)\n    print(f\"Uploaded adapter and tokenizer to {HF_REPO_ID}\")\nelse:\n    print(\"Skipping upload because HF_TOKEN or HF_REPO_ID is missing.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 8. POST TRAINING EVALUATION\nReload the base model with the LoRA adapter and recompute ROUGE L to compare against the baseline."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from peft import PeftModel\n\n# Reload base model and merge LoRA adapter for evaluation\nbase_for_eval = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME_BASE,\n    device_map=\"auto\",\n)\nbase_for_eval = PeftModel.from_pretrained(base_for_eval, adapter_dir)\n\nfinetuned_generation = pipeline(\n    task=\"text-generation\",\n    model=base_for_eval,\n    tokenizer=base_tokenizer,\n    max_new_tokens=128,\n    do_sample=False,\n)\n\nfinetuned_preds = []\nfor example in subset:\n    generated = finetuned_generation(example[\"prompt\"], return_full_text=False)[0][\"generated_text\"]\n    finetuned_preds.append(generated)\n\nfinetuned_scores = rouge.compute(predictions=finetuned_preds, references=refs, use_aggregator=True)\nprint(\"Baseline ROUGE-L:\", baseline_scores.get(\"rougeL\"))\nprint(\"Finetuned ROUGE-L:\", finetuned_scores.get(\"rougeL\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 9. INFERENCE AND GRADIO\nProvide a helper to generate medical answers and expose a minimal Gradio interface."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import gradio as gr\n\ndef generate_medical_answer(question: str, max_new_tokens: int = 128) -> str:\n    formatted_prompt = f\"<think> {system_prompt} Question: {question}\n<response>\"\n    outputs = finetuned_generation(formatted_prompt, return_full_text=False, max_new_tokens=max_new_tokens)\n    return outputs[0][\"generated_text\"]\n\n# Build a simple Gradio interface\ninterface = gr.Interface(\n    fn=generate_medical_answer,\n    inputs=[gr.Textbox(label=\"Medical Question\", lines=4), gr.Slider(32, 256, value=128, step=8, label=\"Max New Tokens\")],\n    outputs=gr.Textbox(label=\"Model Answer\"),\n    title=\"Medical Reasoning with Llama 3\",\n    description=\"LoRA finetuned medical reasoning model built with Unsloth.\",\n)\n\n# Launch only when running interactively\nif __name__ == \"__main__\":\n    interface.launch(share=False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 10. QUALITY CHECK\nRemove unused code, keep comments concise, and verify the notebook runs cleanly from top to bottom in a fresh runtime."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}